# AutoIF Pipeline Configuration
#
# This is the default configuration template for the AutoIF pipeline.
# To start a new experiment:
#   1. Create experiment directory: mkdir -p <OUT_DIR>
#   2. Copy this file: cp config.default.yaml <OUT_DIR>/config.yaml
#   3. Edit <OUT_DIR>/config.yaml with your experiment parameters
#   4. Run: ./pipeline.sh --out-dir <OUT_DIR>
#
# CONFIGURATION SYSTEM:
#   - General configs: Arguments passed to Python scripts/jobs (model, language, etc.)
#   - Job configs: Job-specific parameters and file paths under each job
#   - Cascade: job > global for any parameter

# ==============================================================================
# Experiment Metadata
# ==============================================================================
experiment:
  name: "exp1"
  description: "Baseline IFEval instruction following dataset generation"

# ==============================================================================
# Pipeline Sequence (defines execution order and enabled jobs)
# ==============================================================================
# Set to false to disable a job, or remove it from the list to skip entirely
pipeline:
  - augmentation_generation: true
  - augmentation_postprocessing: true
  - verifiers_generation: true
  - cross_validation_job: true
  - concatenation_job: true
  - responses_generation: true
  - final_dataset_job: true

# ==============================================================================
# vLLM Server Configuration (Optional - for Local File Mode)
# ==============================================================================
# If you want to send the pipeline's requests to a vllm server running on an interactive node
# vllm_server:
#   host: "127.0.0.1"
#   port: 8000

# ==============================================================================
# General Configuration (can be overridden at job level)
# ==============================================================================
# Model parameters
model: "meta-llama/Llama-3.3-70B-Instruct"
max_model_len: 16384
tensor_parallel: 4

# Common parameters
language: "en"
function_timeout: 10
score_threshold: 4
workers: 8

# Environment
venv_dir: "/scratch/project_462000353/adamhrin/dispatcher/examples/autoif/.venv"
hf_home: "/scratch/project_462000353/hf_cache"
batch_size: 1

# ==============================================================================
# Jobs Configuration
# ==============================================================================
# Each job defines job-specific parameters and file paths
#
# Override any general config at job level using the same keyword.
jobs:
  # --------------------------------------------------------------------------
  # Augmentation: Generate augmented instructions from seed instructions
  # --------------------------------------------------------------------------
  augmentation_generation:
    # Augmentation parameters
    seed_file: "/scratch/project_462000353/adamhrin/dispatcher/examples/autoif/data/categorised_instructions.json"
    num_instructions_per_category: 50
    augmentation_input: "aug_input.jsonl"
    augmentation_output: "aug_output.jsonl"

  augmentation_postprocessing:
    augmentation_output: "aug_output.jsonl"
    augmented_instructions: "augmented_instructions.jsonl"
    max_augmented_instructions: 200

  # --------------------------------------------------------------------------
  # Verifiers: Generate and validate verification functions
  # --------------------------------------------------------------------------
  verifiers_generation:
    # Verifiers parameters
    augmented_instructions: "augmented_instructions.jsonl"
    verifiers_input: "verifiers_input.jsonl"
    verifiers_output: "verifiers_output.jsonl"
    min_functions: 1
    min_test_cases: 1
    startup_timeout: 1800
    request_timeout: 1800
    work_timeout: 3600
    num_verifier_generations: 10

  verifiers_postprocessing:
    verifiers_output: "verifiers_output.jsonl"
    verifiers_all: "verifiers_all.jsonl"
    verifiers_filtered: "verifiers_filtered.jsonl"
    function_pass_rate: 0.8

  # --------------------------------------------------------------------------
  # Concatenation: Create queries by concatenating instructions with dataset
  # --------------------------------------------------------------------------
  concatenation_job:
    # Concatenation parameters
    verifiers_filtered: "verifiers_filtered.jsonl"
    verifiers_queries: "verifiers_queries.jsonl"
    queries_dataset: null  # REQUIRED: Path to queries dataset (HF path or local JSONL)
    num_output_lines: 300000
    instructions_per_query: 1
    query_max_len: 200
    messages_format: true
    messages_key: "messages"
    turns: 1
    no_followup: true
    balance_categories: true

  # --------------------------------------------------------------------------
  # Responses: Generate and score responses to queries
  # --------------------------------------------------------------------------
  responses_generation:
    verifiers_queries: "verifiers_queries.jsonl"
    scored_responses: "scored_responses.jsonl"
    startup_timeout: 1800
    request_timeout: 1800
    work_timeout: 3600
    workers: 32

  # --------------------------------------------------------------------------
  # SFT: Build final supervised fine-tuning dataset
  # --------------------------------------------------------------------------
  final_dataset_job:
    scored_responses: "scored_responses.jsonl"
    sft_dataset_dir: "sft_dataset"
