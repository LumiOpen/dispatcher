#!/usr/bin/env python3
"""
Utility to display verification functions in human-readable format.

This script takes a verifiers JSONL file and displays the Python code of verification
functions along with their test cases in a formatted, readable manner.
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any
# Add the parent directory of 'src' to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from src.utils.response_parser import ResponseParser


def format_python_code(code: str) -> str:
    """Format Python code string for better readability."""
    # Remove leading/trailing whitespace and normalize line endings
    code = code.strip()
    
    # Split by lines and clean up
    lines = code.split('\n')
    formatted_lines = []
    
    for line in lines:
        # Remove trailing whitespace but preserve indentation
        formatted_lines.append(line.rstrip())
    
    return '\n'.join(formatted_lines)


def print_function_code(func_code: str, func_number: int, total_funcs: int) -> None:
    """Print a single function code with formatting."""
    print(f"\n{'='*60}")
    print(f"Function {func_number}/{total_funcs}")
    print('='*60)
    print(format_python_code(func_code))


def print_test_cases(cases: List[Dict[str, Any]]) -> None:
    """Print test cases in a readable format."""
    if not cases:
        return
    
    print(f"\n{'='*60}")
    print("Test Cases")
    print('='*60)
    
    for i, case in enumerate(cases, 1):
        input_val = case.get('input', '')
        output_val = case.get('output', '')
        
        print(f"\nTest Case {i}:")
        print(f"  Input:  {repr(input_val)}")
        print(f"  Output: {output_val}")

def parse_raw_verifier_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """Parse raw verifier data into processed format."""
    parser = ResponseParser()
    
    # Extract instruction information from 'original' object
    original = raw_data.get('original', {})
    instruction_id = original.get('instruction_id', raw_data.get('instruction_id', ''))
    instruction = original.get('instruction', raw_data.get('instruction', ''))
    
    # Parse responses into eval_funcs and cases
    responses = raw_data.get('responses', [])
    eval_funcs = []
    cases = []
    
    for response in responses:
        parsed_result, _ = parser.parse_function_and_cases(response, instruction_id)
        if parsed_result:
            if 'func' in parsed_result:
                eval_funcs.append(parsed_result['func'])
            if 'cases' in parsed_result:
                cases.extend(parsed_result['cases'])
    
    # Create processed format
    processed_data = {
        'instruction_id': instruction_id,
        'instruction': instruction,
        'eval_func': eval_funcs,
        'cases': cases
    }
    
    # Copy over filtering information if present
    if 'filtered' in raw_data:
        processed_data['filtered'] = raw_data['filtered']
    if 'reason' in raw_data:
        processed_data['reason'] = raw_data['reason']
    
    return processed_data

def is_raw_format(data: Dict[str, Any]) -> bool:
    """Determine if the data is in raw format (has 'original' and 'responses') or processed format."""
    return 'original' in data and 'responses' in data

def generate_python_file(verifier_data: Dict[str, Any], output_path: str, max_func: int = None) -> None:
    """Generate a Python file with numbered evaluation functions and test examples."""
    eval_funcs = verifier_data.get('eval_func', [])
    cases = verifier_data.get('cases', [])
    instruction = verifier_data.get('instruction', '')
    filtered = verifier_data.get('filtered', False)
    reason = verifier_data.get('reason', '')
    
    if max_func is not None:
        eval_funcs = eval_funcs[:max_func]
    
    with open(output_path, 'w') as f:
        f.write('"""')
        f.write(f'\nVerification functions for instruction: {instruction}\n')
        
        # Add filtering information
        if 'filtered' in verifier_data and 'reason' in verifier_data:
            if filtered:
                f.write(f'This instruction has been filtered out for a reason: {reason}\n')
            else:
                f.write('This verifier is valid\n')
        
        f.write('Generated by preview_verifiers.py\n')
        f.write('"""\n\n')
        
        # Write all evaluation functions with numbered names
        for i, func_code in enumerate(eval_funcs, 1):
            # Replace function name with numbered version
            formatted_code = format_python_code(func_code)
            # Simple replacement - assumes function is named 'evaluate'
            formatted_code = formatted_code.replace('def evaluate(', f'def evaluate_{i}(')
            
            f.write(formatted_code)
            f.write('\n\n')
        
        # Write example usage section
        if cases:
            f.write('\n# Example usage and test cases\n')
            f.write('if __name__ == "__main__":\n')
            f.write('    # Test cases\n')
            f.write('    test_cases = [\n')
            
            for case in cases:
                input_val = case.get('input', '')
                output_val = case.get('output', '')
                f.write(f'        {{"input": {repr(input_val)}, "output": {output_val}}},\n')
            
            f.write('    ]\n\n')
            f.write(f'    print("Instruction: {instruction}")\n')
            # Generate test code for each function
            for i in range(1, len(eval_funcs) + 1):
                f.write(f'    print("evaluate_{i}")\n')
                f.write('    passed = 0\n')
                f.write('    failed = 0\n')
                f.write('    failed_example = None\n')
                f.write('    \n')
                f.write('    for test_idx, case in enumerate(test_cases, 1):\n')
                f.write('        try:\n')
                f.write(f'            result = evaluate_{i}(case["input"])\n')
                f.write('            if result == case["output"]:\n')
                f.write('                passed += 1\n')
                f.write('            else:\n')
                f.write('                failed += 1\n')
                f.write('                if failed_example is None:\n')
                f.write('                    failed_example = f"  input={case[\'input\']}, got={result}, expected={case[\'output\']}"\n')
                f.write('        except Exception as e:\n')
                f.write('            failed += 1\n')
                f.write('            if failed_example is None:\n')
                f.write('                failed_example = f"  input={case[\'input\']}, ERROR: {e}, expected={case[\'output\']}"\n')
                f.write('    \n')
                f.write('    print(f"  Test Cases: {passed} passed, {failed} failed")\n')
                f.write('    if failed_example:\n')
                f.write('        print("  Example of failure:")\n')
                f.write('        print(failed_example)\n')
                f.write('    print()\n')


def main():
    parser = argparse.ArgumentParser(
        description="Display verification functions in human-readable format"
    )
    parser.add_argument(
        "verifiers_file",
        help="Path to the verifiers JSONL file"
    )
    parser.add_argument(
        "--instruction_id",
        required=True,
        help="Instruction ID to display"
    )
    parser.add_argument(
        "--max_func",
        type=int,
        help="Maximum number of functions to display (default: all)"
    )
    parser.add_argument(
        "--tofile",
        action="store_true",
        help="Output to a Python file instead of console"
    )
    
    args = parser.parse_args()
    
    # Check if input file exists
    if not os.path.exists(args.verifiers_file):
        print(f"Error: File '{args.verifiers_file}' does not exist", file=sys.stderr)
        sys.exit(1)
    
    # Find the verifier with the specified instruction_id
    verifier_data = None
    try:
        with open(args.verifiers_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # Handle both raw and processed formats
                    if is_raw_format(data):
                        # Raw format: check instruction_id in 'original'
                        original = data.get('original', {})
                        if original.get('instruction_id') == args.instruction_id:
                            verifier_data = parse_raw_verifier_data(data)
                            break
                    else:
                        # Processed format: check instruction_id directly
                        if data.get('instruction_id') == args.instruction_id:
                            verifier_data = data
                            break
                            
                except json.JSONDecodeError as e:
                    print(f"Warning: Invalid JSON on line {line_num}: {e}", file=sys.stderr)
                    continue
    
    except Exception as e:
        print(f"Error reading file: {e}", file=sys.stderr)
        sys.exit(1)
    
    if verifier_data is None:
        print(f"Error: No verifier found with instruction_id '{args.instruction_id}'", file=sys.stderr)
        sys.exit(1)

    eval_funcs = verifier_data.get('eval_func', [])
    cases = verifier_data.get('cases', [])
    instruction = verifier_data.get('instruction', '')
    
    if not eval_funcs:
        print("Warning: No evaluation functions found", file=sys.stderr)
        return
    
    # Limit functions if max_func is specified
    if args.max_func is not None:
        eval_funcs = eval_funcs[:args.max_func]
    
    if args.tofile:
        # Generate output filename
        input_path = Path(args.verifiers_file)
        output_filename = f"{input_path.stem}_{args.instruction_id}.py"
        output_path = input_path.parent / output_filename
        
        generate_python_file(verifier_data, str(output_path), args.max_func)
        print(f"Output written to: {output_path}")
    
    else:
        # Print to console
        print(f"Instruction ID: {args.instruction_id}")
        print(f"Instruction: {instruction}")
        
        # Add filtering information
        if 'filtered' in verifier_data and 'reason' in verifier_data:
            filtered = verifier_data.get('filtered', False)
            reason = verifier_data.get('reason', '')
            if filtered:
                print(f"This instruction has been filtered out for a reason: {reason}")
            else:
                print("This verifier is valid")
        
        print(f"Total functions: {len(eval_funcs)}")
        
        if args.max_func is not None:
            print(f"Displaying first {args.max_func} functions")
        
        # Display each function
        for i, func_code in enumerate(eval_funcs, 1):
            print_function_code(func_code, i, len(eval_funcs))
        
        # Display test cases
        print_test_cases(cases)
        
        print(f"\n{'='*60}")
        print("Summary")
        print('='*60)
        print(f"Displayed {len(eval_funcs)} evaluation function(s)")
        print(f"Found {len(cases)} test case(s)")


if __name__ == "__main__":
    main()
