#!/usr/bin/env python3
"""
Utility to display verification functions in human-readable format.

This script takes a verifiers JSONL file and displays the Python code of verification
functions along with their test cases in a formatted, readable manner.
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any


def format_python_code(code: str) -> str:
    """Format Python code string for better readability."""
    # Remove leading/trailing whitespace and normalize line endings
    code = code.strip()
    
    # Split by lines and clean up
    lines = code.split('\n')
    formatted_lines = []
    
    for line in lines:
        # Remove trailing whitespace but preserve indentation
        formatted_lines.append(line.rstrip())
    
    return '\n'.join(formatted_lines)


def print_function_code(func_code: str, func_number: int, total_funcs: int) -> None:
    """Print a single function code with formatting."""
    print(f"\n{'='*60}")
    print(f"Function {func_number}/{total_funcs}")
    print('='*60)
    print(format_python_code(func_code))


def print_test_cases(cases: List[Dict[str, Any]]) -> None:
    """Print test cases in a readable format."""
    if not cases:
        return
    
    print(f"\n{'='*60}")
    print("Test Cases")
    print('='*60)
    
    for i, case in enumerate(cases, 1):
        input_val = case.get('input', '')
        output_val = case.get('output', '')
        
        print(f"\nTest Case {i}:")
        print(f"  Input:  {repr(input_val)}")
        print(f"  Output: {output_val}")


def generate_python_file(verifier_data: Dict[str, Any], output_path: str, max_func: int = None) -> None:
    """Generate a Python file with numbered evaluation functions and test examples."""
    eval_funcs = verifier_data.get('eval_func', [])
    cases = verifier_data.get('cases', [])
    instruction = verifier_data.get('instruction', '')
    filtered = verifier_data.get('filtered', False)
    reason = verifier_data.get('reason', '')
    
    if max_func is not None:
        eval_funcs = eval_funcs[:max_func]
    
    with open(output_path, 'w') as f:
        f.write('"""')
        f.write(f'\nVerification functions for instruction: {instruction}\n')
        
        # Add filtering information
        if 'filtered' in verifier_data and 'reason' in verifier_data:
            if filtered:
                f.write(f'This instruction has been filtered out for a reason: {reason}\n')
            else:
                f.write('This verifier is valid\n')
        
        f.write('Generated by preview_verifiers.py\n')
        f.write('"""\n\n')
        
        # Write all evaluation functions with numbered names
        for i, func_code in enumerate(eval_funcs, 1):
            # Replace function name with numbered version
            formatted_code = format_python_code(func_code)
            # Simple replacement - assumes function is named 'evaluate'
            formatted_code = formatted_code.replace('def evaluate(', f'def evaluate_{i}(')
            
            f.write(formatted_code)
            f.write('\n\n')
        
        # Write example usage section
        if cases:
            f.write('\n# Example usage and test cases\n')
            f.write('if __name__ == "__main__":\n')
            f.write('    # Test cases\n')
            f.write('    test_cases = [\n')
            
            for case in cases:
                input_val = case.get('input', '')
                output_val = case.get('output', '')
                f.write(f'        {{"input": {repr(input_val)}, "expected": {output_val}}},\n')
            
            f.write('    ]\n\n')
            
            # Generate test code for each function
            for i in range(1, len(eval_funcs) + 1):
                f.write(f'    print("Testing evaluate_{i}:")\n')
                f.write('    for case in test_cases:\n')
                f.write(f'        result = evaluate_{i}(case["input"])\n')
                f.write('        status = "PASS" if result == case["expected"] else "FAIL"\n')
                f.write('        print(f"  {case[\\"input\\"][:50]...} -> {result} ({status})")\n')
                f.write('    print()\n\n')


def main():
    parser = argparse.ArgumentParser(
        description="Display verification functions in human-readable format"
    )
    parser.add_argument(
        "verifiers_file",
        help="Path to the verifiers JSONL file"
    )
    parser.add_argument(
        "--instruction_id",
        required=True,
        help="Instruction ID to display"
    )
    parser.add_argument(
        "--max_func",
        type=int,
        help="Maximum number of functions to display (default: all)"
    )
    parser.add_argument(
        "--tofile",
        action="store_true",
        help="Output to a Python file instead of console"
    )
    
    args = parser.parse_args()
    
    # Check if input file exists
    if not os.path.exists(args.verifiers_file):
        print(f"Error: File '{args.verifiers_file}' does not exist", file=sys.stderr)
        sys.exit(1)
    
    # Find the verifier with the specified instruction_id
    verifier_data = None
    try:
        with open(args.verifiers_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    if data.get('instruction_id') == args.instruction_id:
                        verifier_data = data
                        break
                except json.JSONDecodeError as e:
                    print(f"Warning: Invalid JSON on line {line_num}: {e}", file=sys.stderr)
                    continue
    
    except Exception as e:
        print(f"Error reading file: {e}", file=sys.stderr)
        sys.exit(1)
    
    if verifier_data is None:
        print(f"Error: No verifier found with instruction_id '{args.instruction_id}'", file=sys.stderr)
        sys.exit(1)
    
    eval_funcs = verifier_data.get('eval_func', [])
    cases = verifier_data.get('cases', [])
    instruction = verifier_data.get('instruction', '')
    
    if not eval_funcs:
        print("Warning: No evaluation functions found", file=sys.stderr)
        return
    
    # Limit functions if max_func is specified
    if args.max_func is not None:
        eval_funcs = eval_funcs[:args.max_func]
    
    if args.tofile:
        # Generate output filename
        input_path = Path(args.verifiers_file)
        output_filename = f"{input_path.stem}_{args.instruction_id}.py"
        output_path = input_path.parent / output_filename
        
        generate_python_file(verifier_data, str(output_path), args.max_func)
        print(f"Output written to: {output_path}")
    
    else:
        # Print to console
        print(f"Instruction ID: {args.instruction_id}")
        print(f"Instruction: {instruction}")
        
        # Add filtering information
        if 'filtered' in verifier_data and 'reason' in verifier_data:
            filtered = verifier_data.get('filtered', False)
            reason = verifier_data.get('reason', '')
            if filtered:
                print(f"This instruction has been filtered out for a reason: {reason}")
            else:
                print("This verifier is valid")
        
        print(f"Total functions: {len(eval_funcs)}")
        
        if args.max_func is not None:
            print(f"Displaying first {args.max_func} functions")
        
        # Display each function
        for i, func_code in enumerate(eval_funcs, 1):
            print_function_code(func_code, i, len(eval_funcs))
        
        # Display test cases
        print_test_cases(cases)
        
        print(f"\n{'='*60}")
        print("Summary")
        print('='*60)
        print(f"Displayed {len(eval_funcs)} evaluation function(s)")
        print(f"Found {len(cases)} test case(s)")


if __name__ == "__main__":
    main()
